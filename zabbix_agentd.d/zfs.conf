# [This file is part of the zabbix-agent-osso package]

# ZFS pool discovery
UserParameter=zfs.pool.discovery, sudo /sbin/zpool list -Ho name | awk 'BEGIN{print "["}{if(NR>1)printf ",";print "{\"{#POOLNAME}\":\"" $1 "\"}"}END{print "]"}'
# ZFS pool json
UserParameter=zfs.pool.json, if v=$(sudo /sbin/zpool status 2>/dev/null) && echo "$v" | awk -f /etc/zabbix/scripts/zfs.status >/var/lib/zabbix/scripts/zfs.status.new; then mv /var/lib/zabbix/scripts/zfs.status.new /var/lib/zabbix/scripts/zfs.status && cat /var/lib/zabbix/scripts/zfs.status; else echo ZBX_NOTSUPPORTED; fi
UserParameter=zfs.pool.changed, n=0; if grep -q '^[1-9][0-9][0-9]' /proc/uptime && test -f /var/lib/zabbix/scripts/zfs.status; then if test -f /var/lib/zabbix/scripts/zfs.status.saved; then cmp -s /var/lib/zabbix/scripts/zfs.status.saved /var/lib/zabbix/scripts/zfs.status || n=1; else cat /var/lib/zabbix/scripts/zfs.status >/var/lib/zabbix/scripts/zfs.status.saved; fi; fi; echo $n
# ZFS filesystem/dataset/"fileset" discovery. We skip all names that end
# in "/[0-9a-f]{64}(-init)?" because we assume them to be "docker" generated
# temp filesystems.
UserParameter=zfs.dataset.discovery, sudo /sbin/zfs list -Ho name | grep -vE '/[0-9a-f]{64}(-init)?$' | awk 'BEGIN{print "["} {if(i)printf ",";i=1;print "{\"{#DATASETNAME}\":\"" $1 "\"}"} END{print "]"}'
# We do want counts of the datasets (by filesystem/volume), including the
# docker/containerd filesystems. These should generally complete within a few
# seconds. But when there are more, it will take longer. We'll have to use a
# cached lookup.
UserParameter=zfs.filesystem.count, awk '{if($1=="filesystem.count"){d=1;print $2}} END{if(!d)print "ZBX_NOTSUPPORTED"}' /var/lib/zabbix/scripts/zfs.cache
UserParameter=zfs.volume.count, awk '{if($1=="volume.count"){d=1;print $2}} END{if(!d)print "ZBX_NOTSUPPORTED"}' /var/lib/zabbix/scripts/zfs.cache

# ZFS module version
UserParameter=zfs.version, cat /sys/module/zfs/version
# ZFS module/global parameters/config (e.g. 'zfs_arc_max')
UserParameter=zfs.config[*], cat /sys/module/zfs/parameters/$1

# ZFS custom arc config checks:
# - arc_max should be set in the config; the default is always wrong
# - that config should be applied (be in max.file==max.sys)
# - the value should be >=1GB
UserParameter=zfs.arc.max.file, n=$(sed -e '/^options zfs .*zfs_arc_max=/!d;s/.*zfs_arc_max=//;s/[^0-9].*//' /etc/modprobe.d/*.conf); echo ${n:-0}
UserParameter=zfs.arc.max.sys, cat /sys/module/zfs/parameters/zfs_arc_max

# ZFS total memory used: the sum of the SPL slab allocator's statistics.
# "There are a few things not included in that, like the page cache used by
# mmap(). But you can expect it to be relatively accurate."
#UserParameter=zfs.memory.used, awk '{if(NR>2){n+=$3}}END{print n}' /proc/spl/kmem/slab
# (If the value exceeds 2GB, awk will use e+06 notation. Zabbix won't grok that.)
UserParameter=zfs.memory.used, awk '{if(NR>2){n+=$3}}END{printf "%d000\n",((n+999)/1000)}' /proc/spl/kmem/slab
# Total memory available, in case you want to calculate percentages.
UserParameter=zfs.memory.exists, free -bw | awk '/^Mem:/{print $2}'
# ZFS ARC stats from /proc/spl/kstat/zfs/arcstats (e.g. 'size', 'mru_hits', 'misses')
UserParameter=zfs.arcstats[*], awk '/^$1/{print $''3}' /proc/spl/kstat/zfs/arcstats

# ZFS pool health (takes a {#POOLNAME}, returns a string like "ONLINE" or "DEGRADED"(?))
UserParameter=zfs.zpool.health[*], sudo /sbin/zpool list -Ho health $1

# ZFS filesystem options (takes a {#DATASETNAME} and a property name, like 'size')
UserParameter=zfs.dataset.info[*], sudo /sbin/zfs get -Hpo value $2 $1

# ZFS stats
UserParameter=zfs.zpool.stats.json[*], awk '{if(NR==3){print "{\"rbytes\":" $''1 ",\"wbytes\":" $''2 ",\"rops\":" $''3 ",\"wops\":" $''4 "}"}}' /proc/spl/kstat/zfs/$1/io
