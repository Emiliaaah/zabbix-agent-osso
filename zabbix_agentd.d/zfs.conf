# [This file is part of the zabbix-agent-osso package]

# ZFS pool discovery
UserParameter=zfs.pool.discovery, sudo /sbin/zpool list -Ho name | awk 'BEGIN{print "["}{if(NR>1)printf ",";print "{\"{#POOLNAME}\":\"" $1 "\"}"}END{print "]"}'
# ZFS pool json
UserParameter=zfs.pool.json, if v=$(sudo /sbin/zpool status 2>/dev/null) && echo "$v" | awk -f /etc/zabbix/scripts/zfs.status >/var/lib/zabbix/scripts/zfs.status.new; then mv /var/lib/zabbix/scripts/zfs.status.new /var/lib/zabbix/scripts/zfs.status && cat /var/lib/zabbix/scripts/zfs.status; else echo ZBX_NOTSUPPORTED; fi
UserParameter=zfs.pool.changed, n=0; if grep -q '^[1-9][0-9][0-9]' /proc/uptime && test -f /var/lib/zabbix/scripts/zfs.status; then if test -f /var/lib/zabbix/scripts/zfs.status.saved; then cmp -s /var/lib/zabbix/scripts/zfs.status.saved /var/lib/zabbix/scripts/zfs.status || n=1; else cat /var/lib/zabbix/scripts/zfs.status >/var/lib/zabbix/scripts/zfs.status.saved; fi; fi; echo $n
# ZFS filesystem/dataset/"fileset" discovery. See the "ZFS dataset discovery"
# regexp for includes/excludes.
UserParameter=zfs.dataset.discovery, sudo /sbin/zfs list -Ho name | awk 'BEGIN{print "["} {if(i)printf ",";i=1;print "{\"{#DATASETNAME}\":\"" $1 "\"}"} END{print "]"}'
# We do want counts of the datasets (by filesystem/volume), including the
# docker/containerd filesystems. These should generally complete within a few
# seconds. But when there are more, it will take longer. We'll have to use a
# cached lookup.
UserParameter=zfs.filesystem.count, awk '{if($1=="filesystem.count"){d=1;print $2}} END{if(!d)print "ZBX_NOTSUPPORTED"}' /var/lib/zabbix/scripts/zfs.cache
UserParameter=zfs.volume.count, awk '{if($1=="volume.count"){d=1;print $2}} END{if(!d)print "ZBX_NOTSUPPORTED"}' /var/lib/zabbix/scripts/zfs.cache

# ZFS module version
UserParameter=zfs.version, cat /sys/module/zfs/version
# ZFS module/global parameters/config (e.g. 'zfs_arc_max')
UserParameter=zfs.config[*], cat /sys/module/zfs/parameters/$1

# ZFS custom arc config checks:
# - arc_max should be set in the config; the default is always wrong
# - that config should be applied (be in max.file==max.sys)
# - the value should be >=1GB
UserParameter=zfs.arc.max.file, n=$(sed -e '/^options zfs .*zfs_arc_max=/!d;s/.*zfs_arc_max=//;s/[^0-9].*//' /etc/modprobe.d/*.conf); echo ${n:-0}
UserParameter=zfs.arc.max.sys, cat /sys/module/zfs/parameters/zfs_arc_max

# ZFS total memory used: the sum of the SPL slab allocator's statistics.
# "There are a few things not included in that, like the page cache used by
# mmap(). But you can expect it to be relatively accurate."
#UserParameter=zfs.memory.used, awk '{if(NR>2){n+=$3}}END{print n}' /proc/spl/kmem/slab
# (If the value exceeds 2GB, awk will use e+06 notation. Zabbix won't grok that.)
UserParameter=zfs.memory.used, awk '{if(NR>2){n+=$3}}END{printf "%d000\n",((n+999)/1000)}' /proc/spl/kmem/slab
# Total memory available, in case you want to calculate percentages.
UserParameter=zfs.memory.exists, free -bw | awk '/^Mem:/{print $2}'
# ZFS ARC stats from /proc/spl/kstat/zfs/arcstats (e.g. 'size', 'mru_hits', 'misses')
UserParameter=zfs.arcstats[*], awk '/^$1/{print $''3}' /proc/spl/kstat/zfs/arcstats

# ZFS pool health (takes a {#POOLNAME}, returns a string like "ONLINE" or "DEGRADED"(?))
UserParameter=zfs.zpool.health[*], sudo /sbin/zpool list -Ho health $1

# ZFS filesystem options (takes a {#DATASETNAME} and a property name, like 'size')
UserParameter=zfs.dataset.info[*], sudo /sbin/zfs get -Hpo value $2 $1

# ZFS stats
# Remove pool io kstats (#12212) Â· openzfs/zfs@371f88d ("pool wide locks caused bottlenecks")
# > # zpool status -vPL rpool | sed -ne 's/^[[:blank:]]*\/dev\/\([^[:blank:]]*\).*/\1/p'
# > sda4
# > sdb4
# > # grep ' sd[ab]4 ' /proc/diskstats
# >   8      20 sdb4 1683400 17 1300860 77917 19184618 102070 682901320 2075811 0 6157040 2299233 780202 0 624328952 145504 0 0
# >   8       4 sda4 1683907 17 1438316 86822 18987834 124569 682901320 2144396 0 6164080 2424957 780202 0 624328952 193738 0 0
# >                   ^reads ^rmrg ^rsect ^rms ^writes  ^wmrg ^wsect    ^wms    ^io ^ioms ^weightio ^dsc ^dmrg ^dsect ^dms  ^? ^?
# NOTE: Awk does only 32 bits integers. So we truncate them to 30 bits before summing.
# And we assume 512-byte sectors. The ops will be far more useful than the byte counts.
# So, if /proc/spl/kstat/zfs/<POOL>/io is gone, we sum all the values of the
# partitions in the pool. Then we divide the sector writes by 2, so we get (in
# the case of 512k sectors) kilobyte reads/writes (we then add 000 to the output).
# If the sectors are larger (4K or 8K), the read/write byte counts will be a factor
# of 8 or 16 too low.
UserParameter=zfs.zpool.stats.json[*], if test -f /proc/spl/kstat/zfs/$1/io; then awk '{if(NR==3){print "{\"rbytes\":" $''1 ",\"wbytes\":" $''2 ",\"rops\":" $''3 ",\"wops\":" $''4 "}"}}' /proc/spl/kstat/zfs/$1/io; else awk -vMAXINT=1073741824 -vPS="$(zpool status -vPL $1 | sed -ne 's/^[[:blank:]]*\/dev\/\([^[:blank:]]*\).*/\1/p')" 'BEGIN{PSA[0];split(PS,PSA);for(i=1;i<=length(PSA);++i)PSD[PSA[i]]=1;} END{printf "{\"rbytes\":%lu000,\"wbytes\":%lu000,\"rops\":%lu,\"wops\":%lu}\n",(rs/2),(ws/2),r,w;} {if($''3 in PSD){r+=($''4%MAXINT);rs+=($''6%MAXINT);w+=($''8%MAXINT);ws+=($''10%MAXINT)}}' /proc/diskstats; fi
